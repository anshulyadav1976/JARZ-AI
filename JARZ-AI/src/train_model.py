"""
Spatio-Temporal Rental Valuation Model Training
================================================
Trains a quantile regression model to predict rental values with:
- P10/P50/P90 confidence bands
- Spatial features (neighbor effects)
- Temporal features (growth trends)
- SHAP explainability

Uses data generated by Get_Data.py
"""

import numpy as np
import pandas as pd
from pathlib import Path
from typing import Optional, Tuple, Dict, Any
import pickle
import warnings
warnings.filterwarnings('ignore')

# ============ CONFIGURATION ============
DATA_DIR = Path("data/raw")
MODEL_DIR = Path("models")
MODEL_DIR.mkdir(parents=True, exist_ok=True)

FEATURES_PATH = DATA_DIR / "district_training_data.parquet"
TIMESERIES_PATH = DATA_DIR / "district_growth_timeseries.parquet"
MODEL_PATH = MODEL_DIR / "rent_quantile_model.pkl"

# Target variable for rent prediction
TARGET_COL = "rent_demand_mean_pcm"
FALLBACK_TARGET = "rent_avg_pcm"

# Quantiles for prediction bands
QUANTILES = [0.1, 0.5, 0.9]  # P10, P50, P90

# UK district centroid coordinates (approximate) for spatial features
# In production, you'd load this from a proper geocoding service
DISTRICT_COORDS = {
    # London
    "E1": (51.5152, -0.0726), "E2": (51.5294, -0.0556), "E3": (51.5282, -0.0214),
    "E4": (51.6277, -0.0069), "E5": (51.5563, -0.0516), "E6": (51.5222, 0.0517),
    "E7": (51.5464, 0.0227), "E8": (51.5432, -0.0553), "E9": (51.5467, -0.0396),
    "E10": (51.5647, -0.0130), "E11": (51.5722, 0.0075), "E14": (51.5074, -0.0235),
    "EC1": (51.5246, -0.0990), "EC2": (51.5194, -0.0850), "EC3": (51.5128, -0.0825),
    "EC4": (51.5136, -0.1031), "N1": (51.5385, -0.1024), "N2": (51.5876, -0.1647),
    "N3": (51.6019, -0.1913), "N4": (51.5679, -0.1035), "N5": (51.5549, -0.0966),
    "N6": (51.5728, -0.1458), "N7": (51.5559, -0.1163), "N8": (51.5859, -0.1182),
    "NW1": (51.5340, -0.1465), "NW2": (51.5596, -0.2239), "NW3": (51.5504, -0.1772),
    "NW4": (51.5848, -0.2292), "NW5": (51.5535, -0.1424), "NW6": (51.5452, -0.1989),
    "NW7": (51.6125, -0.2436), "NW8": (51.5311, -0.1712), "NW9": (51.5876, -0.2587),
    "NW10": (51.5389, -0.2450), "NW11": (51.5769, -0.1971),
    "SE1": (51.5014, -0.0887), "SE2": (51.4868, 0.1006), "SE3": (51.4637, 0.0173),
    "SE4": (51.4542, -0.0396), "SE5": (51.4743, -0.0889), "SE6": (51.4410, -0.0184),
    "SE7": (51.4817, 0.0373), "SE8": (51.4775, -0.0268), "SE9": (51.4456, 0.0714),
    "SE10": (51.4826, 0.0063), "SE11": (51.4906, -0.1068),
    "SW1": (51.4947, -0.1411), "SW2": (51.4517, -0.1197), "SW3": (51.4893, -0.1678),
    "SW4": (51.4572, -0.1377), "SW5": (51.4911, -0.1889), "SW6": (51.4750, -0.2039),
    "SW7": (51.4946, -0.1769), "SW8": (51.4756, -0.1294), "SW9": (51.4675, -0.1139),
    "SW10": (51.4826, -0.1876), "SW11": (51.4627, -0.1639),
    "W1": (51.5145, -0.1437), "W2": (51.5137, -0.1813), "W3": (51.5098, -0.2666),
    "W4": (51.4901, -0.2617), "W5": (51.5099, -0.3065), "W6": (51.4915, -0.2287),
    "W7": (51.5079, -0.3210), "W8": (51.5007, -0.1942), "W9": (51.5264, -0.1894),
    "W10": (51.5242, -0.2109), "W11": (51.5145, -0.2056), "W12": (51.5042, -0.2392),
    "WC1": (51.5214, -0.1228), "WC2": (51.5118, -0.1223),
    # Major cities
    "M1": (53.4808, -2.2426), "M2": (53.4796, -2.2484), "M3": (53.4841, -2.2530),
    "M4": (53.4844, -2.2300), "M14": (53.4532, -2.2166), "M15": (53.4642, -2.2610),
    "M16": (53.4568, -2.2839), "M20": (53.4283, -2.2266), "M21": (53.4317, -2.2697),
    "B1": (52.4797, -1.8998), "B2": (52.4768, -1.8964), "B3": (52.4831, -1.8913),
    "B4": (52.4852, -1.8876), "B5": (52.4729, -1.8971), "B15": (52.4606, -1.9304),
    "L1": (53.4084, -2.9916), "L2": (53.4061, -2.9864), "L3": (53.4100, -2.9808),
    "LS1": (53.7997, -1.5491), "LS2": (53.8009, -1.5391), "LS6": (53.8200, -1.5558),
    "BS1": (51.4545, -2.5879), "BS2": (51.4622, -2.5693), "BS3": (51.4424, -2.5942),
    "G1": (55.8609, -4.2514), "G2": (55.8642, -4.2528), "G3": (55.8677, -4.2608),
    "EH1": (55.9505, -3.1916), "EH2": (55.9508, -3.2014), "EH3": (55.9458, -3.2119),
}


def load_data() -> Tuple[pd.DataFrame, pd.DataFrame]:
    """Load training data from parquet files."""
    print("Loading data...")
    
    if not FEATURES_PATH.exists():
        raise FileNotFoundError(
            f"Features file not found: {FEATURES_PATH}\n"
            "Run Get_Data.py first to collect training data."
        )
    
    df_features = pd.read_parquet(FEATURES_PATH)
    print(f"  Loaded {len(df_features)} districts from {FEATURES_PATH}")
    
    df_timeseries = pd.DataFrame()
    if TIMESERIES_PATH.exists():
        df_timeseries = pd.read_parquet(TIMESERIES_PATH)
        print(f"  Loaded {len(df_timeseries)} time points from {TIMESERIES_PATH}")
    
    return df_features, df_timeseries


def add_spatial_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Add spatial features based on geographic neighbors.
    
    Spatial dependence: nearby areas' rents influence each other.
    """
    print("Adding spatial features...")
    
    # Add coordinates
    df = df.copy()
    df["lat"] = df["district"].map(lambda x: DISTRICT_COORDS.get(x, (None, None))[0])
    df["lon"] = df["district"].map(lambda x: DISTRICT_COORDS.get(x, (None, None))[1])
    
    # For districts with coordinates, compute spatial lag features
    has_coords = df["lat"].notna()
    
    if has_coords.sum() < 5:
        print("  Warning: Few districts have coordinates, using region-based features instead")
        # Extract region from district code (first letters)
        df["region"] = df["district"].str.extract(r'^([A-Z]+)', expand=False)
        
        # Compute region-level averages as spatial features
        for col in ["rent_demand_mean_pcm", "rent_avg_pcm", "growth_latest_avg_price"]:
            if col in df.columns:
                region_avg = df.groupby("region")[col].transform("mean")
                df[f"spatial_{col}_region_avg"] = region_avg
        
        return df
    
    # Compute K-nearest neighbor features
    from scipy.spatial.distance import cdist
    
    coords = df.loc[has_coords, ["lat", "lon"]].values
    districts_with_coords = df.loc[has_coords, "district"].values
    
    # Distance matrix
    if len(coords) > 1:
        dist_matrix = cdist(coords, coords, metric="euclidean")
        
        # For each district, find K nearest neighbors (K=5)
        K = min(5, len(coords) - 1)
        
        neighbor_features = {}
        for i, dist_col in enumerate(df.loc[has_coords].index):
            distances = dist_matrix[i]
            # Get indices of K nearest (excluding self)
            nearest_idx = np.argsort(distances)[1:K+1]
            
            # Compute average of neighbors for key features
            for col in ["rent_demand_mean_pcm", "rent_avg_pcm", "growth_latest_avg_price"]:
                if col in df.columns:
                    neighbor_vals = df.loc[has_coords].iloc[nearest_idx][col].dropna()
                    if len(neighbor_vals) > 0:
                        if dist_col not in neighbor_features:
                            neighbor_features[dist_col] = {}
                        neighbor_features[dist_col][f"spatial_{col}_neighbor_avg"] = neighbor_vals.mean()
        
        # Add to dataframe
        for idx, feats in neighbor_features.items():
            for col, val in feats.items():
                df.loc[idx, col] = val
    
    print(f"  Added spatial features for {has_coords.sum()} districts with coordinates")
    return df


def add_temporal_features(df: pd.DataFrame, df_ts: pd.DataFrame) -> pd.DataFrame:
    """
    Add temporal features from time series data.
    
    Temporal dependence: past rent trends predict future rents.
    """
    print("Adding temporal features...")
    df = df.copy()
    
    if df_ts.empty:
        print("  No time series data available, using growth summary features only")
        return df
    
    # Aggregate time series features per district
    ts_features = []
    
    for district in df["district"].unique():
        district_ts = df_ts[df_ts["district"] == district].sort_values(["year", "month"])
        
        if len(district_ts) < 3:
            continue
        
        feats = {"district": district}
        
        # Recent trend (last 6 months vs previous 6 months)
        if len(district_ts) >= 12:
            recent = district_ts["avg_price"].tail(6).mean()
            previous = district_ts["avg_price"].iloc[-12:-6].mean()
            if previous and previous > 0:
                feats["temporal_6mo_growth"] = (recent - previous) / previous * 100
        
        # Volatility (std of pct changes)
        pct_changes = district_ts["pct_change"].dropna()
        if len(pct_changes) > 0:
            feats["temporal_volatility"] = pct_changes.std()
            feats["temporal_avg_monthly_change"] = pct_changes.mean()
        
        # Momentum (recent price vs long-term average)
        if len(district_ts) >= 6:
            recent_price = district_ts["avg_price"].tail(3).mean()
            long_avg = district_ts["avg_price"].mean()
            if long_avg and long_avg > 0:
                feats["temporal_momentum"] = (recent_price - long_avg) / long_avg * 100
        
        # Transaction volume trend
        txn = district_ts["transactions"].dropna()
        if len(txn) >= 6:
            recent_txn = txn.tail(3).mean()
            prev_txn = txn.iloc[-6:-3].mean()
            if prev_txn and prev_txn > 0:
                feats["temporal_txn_trend"] = (recent_txn - prev_txn) / prev_txn * 100
        
        ts_features.append(feats)
    
    if ts_features:
        df_ts_feats = pd.DataFrame(ts_features)
        df = df.merge(df_ts_feats, on="district", how="left")
        print(f"  Added temporal features for {len(ts_features)} districts")
    
    return df


def prepare_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, list]:
    """
    Prepare final feature matrix for training.
    Returns (X, feature_names)
    """
    print("Preparing feature matrix...")
    
    # Select numeric columns as features (exclude target and identifiers)
    exclude_cols = {
        "district", "area_code_type", "error",
        TARGET_COL, FALLBACK_TARGET,
        "lat", "lon", "region"
    }
    
    feature_cols = [
        col for col in df.columns 
        if col not in exclude_cols 
        and df[col].dtype in ["float64", "int64", "float32", "int32"]
    ]
    
    X = df[feature_cols].copy()
    
    # Handle missing values - fill with median
    for col in X.columns:
        if X[col].isna().any():
            median_val = X[col].median()
            if pd.isna(median_val):
                median_val = 0
            X[col] = X[col].fillna(median_val)
    
    print(f"  Feature matrix: {X.shape[0]} samples Ã— {X.shape[1]} features")
    return X, feature_cols


def train_quantile_model(
    X: pd.DataFrame, 
    y: pd.Series, 
    quantiles: list = QUANTILES,
    min_samples: int = 20
) -> Dict[str, Any]:
    """
    Train quantile regression models for P10/P50/P90 predictions.
    
    Uses LightGBM with quantile loss for each quantile.
    Falls back to sklearn GradientBoosting if LightGBM unavailable.
    If data is too sparse, uses synthetic augmentation.
    """
    print(f"\nTraining quantile models for {quantiles}...")
    
    # Remove samples with missing target
    valid_mask = y.notna() & (y > 0)
    X_train = X[valid_mask].copy()
    y_train = y[valid_mask].copy()
    
    print(f"  Training samples: {len(X_train)} (dropped {(~valid_mask).sum()} with missing target)")
    
    # If not enough samples, augment with synthetic data
    if len(X_train) < min_samples:
        print(f"  âš ï¸  Not enough samples ({len(X_train)}), generating synthetic training data...")
        X_train, y_train = generate_synthetic_training_data(X, y)
        print(f"  Augmented to {len(X_train)} samples")
    
    models = {}
    
    try:
        import lightgbm as lgb
        print("  Using LightGBM quantile regression")
        
        for q in quantiles:
            print(f"    Training quantile {q}...")
            model = lgb.LGBMRegressor(
                objective="quantile",
                alpha=q,
                n_estimators=50,
                max_depth=4,
                learning_rate=0.1,
                num_leaves=15,
                min_child_samples=3,
                random_state=42,
                verbose=-1
            )
            model.fit(X_train, y_train)
            models[f"q{int(q*100)}"] = model
            
    except ImportError:
        print("  LightGBM not available, using sklearn GradientBoosting")
        from sklearn.ensemble import GradientBoostingRegressor
        
        for q in quantiles:
            print(f"    Training quantile {q}...")
            model = GradientBoostingRegressor(
                loss="quantile",
                alpha=q,
                n_estimators=50,
                max_depth=3,
                learning_rate=0.1,
                min_samples_leaf=3,
                random_state=42
            )
            model.fit(X_train, y_train)
            models[f"q{int(q*100)}"] = model
    
    # Store feature names and training info
    model_artifact = {
        "models": models,
        "feature_names": list(X_train.columns),
        "quantiles": quantiles,
        "target_col": TARGET_COL,
        "n_train_samples": len(X_train),
        "train_target_stats": {
            "mean": float(y_train.mean()),
            "std": float(y_train.std()),
            "min": float(y_train.min()),
            "max": float(y_train.max()),
        }
    }
    
    return model_artifact


def generate_synthetic_training_data(X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:
    """
    Generate synthetic training data based on UK rental market knowledge.
    
    This is used when API data is too sparse for training.
    Uses realistic rental price distributions for different UK areas.
    """
    np.random.seed(42)
    
    # UK rental market reference data (Â£/month by region type)
    # Based on ONS and Rightmove data
    RENTAL_PROFILES = {
        "prime_london": {"mean": 4500, "std": 1500, "min": 2000, "max": 15000},
        "central_london": {"mean": 2800, "std": 800, "min": 1500, "max": 6000},
        "outer_london": {"mean": 1800, "std": 500, "min": 1000, "max": 3500},
        "major_city": {"mean": 1200, "std": 400, "min": 600, "max": 2500},
        "suburban": {"mean": 1000, "std": 300, "min": 500, "max": 2000},
    }
    
    # District to profile mapping
    DISTRICT_PROFILES = {
        # Prime London
        "W1": "prime_london", "SW1": "prime_london", "SW3": "prime_london",
        "SW7": "prime_london", "W8": "prime_london", "NW8": "prime_london",
        "WC1": "prime_london", "WC2": "prime_london",
        # Central London
        "NW1": "central_london", "NW3": "central_london", "W2": "central_london",
        "W11": "central_london", "SW5": "central_london", "SW6": "central_london",
        "SW10": "central_london", "N1": "central_london", "E1": "central_london",
        "SE1": "central_london", "EC1": "central_london", "EC2": "central_london",
        # Outer London
        "NW2": "outer_london", "NW4": "outer_london", "NW5": "outer_london",
        "NW6": "outer_london", "NW10": "outer_london", "SW4": "outer_london",
        "SW11": "outer_london", "W4": "outer_london", "W6": "outer_london",
        "W12": "outer_london", "N4": "outer_london", "N5": "outer_london",
        "N7": "outer_london", "N16": "outer_london", "N19": "outer_london",
        "E2": "outer_london", "E3": "outer_london", "E8": "outer_london",
        "E14": "outer_london", "E17": "outer_london", "SE5": "outer_london",
        "SE10": "outer_london", "SE11": "outer_london", "SE15": "outer_london",
        "SE16": "outer_london", "SE22": "outer_london",
        # Major cities
        "M1": "major_city", "M14": "major_city", "M20": "major_city",
        "LS1": "major_city", "LS6": "major_city",
    }
    
    # Generate synthetic samples
    n_synthetic = 100
    synthetic_X = []
    synthetic_y = []
    
    for i in range(n_synthetic):
        # Random district profile
        profiles = list(RENTAL_PROFILES.keys())
        weights = [0.1, 0.25, 0.3, 0.2, 0.15]  # Distribution of samples
        profile = np.random.choice(profiles, p=weights)
        profile_data = RENTAL_PROFILES[profile]
        
        # Generate rent
        rent = np.clip(
            np.random.normal(profile_data["mean"], profile_data["std"]),
            profile_data["min"],
            profile_data["max"]
        )
        
        # Generate correlated features
        features = {}
        
        # Property value correlates with rent (roughly 200x annual rent)
        annual_rent = rent * 12
        property_value = annual_rent * (15 + np.random.normal(0, 3))  # yield 5-8%
        
        features["growth_latest_avg_price"] = property_value
        features["sale_demand_mean_price"] = property_value * (0.9 + np.random.random() * 0.2)
        features["sold_price_min_5yrs"] = property_value * 0.6
        features["sold_price_max_5yrs"] = property_value * 1.5
        
        # Rent metrics
        features["rent_demand_mean_pcm"] = rent * (0.95 + np.random.random() * 0.1)
        features["rent_avg_pcm"] = rent * (0.9 + np.random.random() * 0.2)
        features["rent_pcm_min"] = rent * 0.6
        features["rent_pcm_max"] = rent * 1.8
        
        # Listings and properties
        features["total_properties"] = np.random.randint(5000, 50000)
        features["current_rent_listings"] = np.random.randint(10, 500)
        features["current_sale_listings"] = np.random.randint(20, 800)
        features["rent_listing_count"] = np.random.randint(5, 200)
        features["sale_listing_count"] = np.random.randint(10, 300)
        
        # Market metrics
        features["rent_demand_days_on_market"] = np.random.randint(14, 60)
        features["rent_demand_months_inventory"] = np.random.uniform(0.5, 4)
        features["sale_demand_days_on_market"] = np.random.randint(30, 120)
        
        # Growth metrics
        features["growth_5yr_total_pct"] = np.random.normal(20, 15)
        features["growth_avg_yearly_pct"] = features["growth_5yr_total_pct"] / 5
        
        # Crime (inversely correlated with rent in expensive areas)
        base_crime = 500 if profile in ["prime_london", "central_london"] else 300
        features["crime_total_incidents"] = int(base_crime * (0.5 + np.random.random()))
        
        # Fill remaining columns with zeros
        for col in X.columns:
            if col not in features:
                features[col] = 0
        
        synthetic_X.append(features)
        synthetic_y.append(rent)
    
    # Combine with any real data
    valid_mask = y.notna() & (y > 0)
    if valid_mask.sum() > 0:
        X_real = X[valid_mask]
        y_real = y[valid_mask]
        
        df_synthetic = pd.DataFrame(synthetic_X)[X.columns]
        X_combined = pd.concat([X_real, df_synthetic], ignore_index=True)
        y_combined = pd.concat([y_real, pd.Series(synthetic_y)], ignore_index=True)
    else:
        X_combined = pd.DataFrame(synthetic_X)[X.columns]
        y_combined = pd.Series(synthetic_y)
    
    return X_combined, y_combined


def evaluate_model(model_artifact: Dict, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
    """Evaluate model performance."""
    print("\nEvaluating model...")
    
    valid_mask = y.notna() & (y > 0)
    X_eval = X[valid_mask]
    y_eval = y[valid_mask]
    
    if len(X_eval) == 0:
        print("  No valid samples for evaluation")
        return {}
    
    models = model_artifact["models"]
    metrics = {}
    
    # Predictions
    preds = {}
    for name, model in models.items():
        preds[name] = model.predict(X_eval)
    
    # MAE and coverage for each quantile
    from sklearn.metrics import mean_absolute_error, mean_squared_error
    
    if "q50" in preds:
        mae = mean_absolute_error(y_eval, preds["q50"])
        rmse = np.sqrt(mean_squared_error(y_eval, preds["q50"]))
        mape = np.mean(np.abs((y_eval - preds["q50"]) / y_eval)) * 100
        
        metrics["mae"] = mae
        metrics["rmse"] = rmse
        metrics["mape"] = mape
        
        print(f"  P50 (Median) - MAE: Â£{mae:.0f}, RMSE: Â£{rmse:.0f}, MAPE: {mape:.1f}%")
    
    # Check coverage of prediction intervals
    if "q10" in preds and "q90" in preds:
        coverage = np.mean((y_eval >= preds["q10"]) & (y_eval <= preds["q90"]))
        metrics["coverage_80"] = coverage
        print(f"  80% Interval Coverage: {coverage*100:.1f}% (target: 80%)")
    
    # Interval width
    if "q10" in preds and "q90" in preds:
        avg_width = np.mean(preds["q90"] - preds["q10"])
        metrics["avg_interval_width"] = avg_width
        print(f"  Average Interval Width: Â£{avg_width:.0f}")
    
    return metrics


def compute_shap_values(model_artifact: Dict, X: pd.DataFrame) -> Optional[Dict]:
    """Compute SHAP values for model explainability."""
    print("\nComputing SHAP values...")
    
    try:
        import shap
    except ImportError:
        print("  SHAP not installed, skipping explainability")
        return None
    
    # Use P50 model for explanations
    model = model_artifact["models"].get("q50")
    if model is None:
        print("  No P50 model found")
        return None
    
    # Sample if too many rows
    X_sample = X.sample(min(100, len(X)), random_state=42) if len(X) > 100 else X
    
    try:
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X_sample)
        
        # Get feature importance from SHAP
        feature_importance = pd.DataFrame({
            "feature": X_sample.columns,
            "importance": np.abs(shap_values).mean(axis=0)
        }).sort_values("importance", ascending=False)
        
        print("\n  Top 10 Features (SHAP importance):")
        for _, row in feature_importance.head(10).iterrows():
            print(f"    {row['feature']:40s} {row['importance']:.2f}")
        
        return {
            "shap_values": shap_values,
            "feature_importance": feature_importance,
            "base_value": float(explainer.expected_value)
        }
        
    except Exception as e:
        print(f"  SHAP computation failed: {e}")
        return None


def save_model(model_artifact: Dict, metrics: Dict, shap_info: Optional[Dict]):
    """Save trained model to disk."""
    print(f"\nSaving model to {MODEL_PATH}...")
    
    # Add metrics and shap to artifact
    model_artifact["metrics"] = metrics
    if shap_info:
        model_artifact["shap_feature_importance"] = shap_info.get("feature_importance")
        model_artifact["shap_base_value"] = shap_info.get("base_value")
    
    with open(MODEL_PATH, "wb") as f:
        pickle.dump(model_artifact, f)
    
    print(f"  âœ… Model saved: {MODEL_PATH.stat().st_size / 1024:.1f} KB")


def predict(model_artifact: Dict, features: Dict[str, float]) -> Dict[str, float]:
    """
    Make a prediction with the trained model.
    
    Args:
        model_artifact: Loaded model artifact
        features: Dict of feature name -> value
    
    Returns:
        Dict with p10, p50, p90 predictions
    """
    feature_names = model_artifact["feature_names"]
    
    # Build feature vector in correct order
    X = pd.DataFrame([{
        name: features.get(name, 0) 
        for name in feature_names
    }])
    
    predictions = {}
    for name, model in model_artifact["models"].items():
        pred = model.predict(X)[0]
        predictions[name] = float(pred)
    
    return {
        "p10": predictions.get("q10", 0),
        "p50": predictions.get("q50", 0),
        "p90": predictions.get("q90", 0),
        "unit": "GBP/month"
    }


def load_model(path: Path = MODEL_PATH) -> Dict:
    """Load a trained model from disk."""
    with open(path, "rb") as f:
        return pickle.load(f)


# ============ MAIN EXECUTION ============
if __name__ == "__main__":
    print("=" * 70)
    print("SPATIO-TEMPORAL RENTAL VALUATION MODEL TRAINING")
    print("=" * 70)
    print("\nThis trains a quantile regression model for rent prediction")
    print("with P10/P50/P90 confidence bands and SHAP explainability.")
    print("=" * 70)
    
    # 1. Load data
    df_features, df_timeseries = load_data()
    
    # 2. Add spatial features
    df_features = add_spatial_features(df_features)
    
    # 3. Add temporal features
    df_features = add_temporal_features(df_features, df_timeseries)
    
    # 4. Determine target column
    if TARGET_COL in df_features.columns and df_features[TARGET_COL].notna().sum() > 10:
        target_col = TARGET_COL
    elif FALLBACK_TARGET in df_features.columns and df_features[FALLBACK_TARGET].notna().sum() > 10:
        target_col = FALLBACK_TARGET
    else:
        # Try to find any rent column
        rent_cols = [c for c in df_features.columns if "rent" in c.lower() and "pcm" in c.lower()]
        for col in rent_cols:
            if df_features[col].notna().sum() > 5:
                target_col = col
                break
        else:
            raise ValueError("No suitable target column found. Need more training data.")
    
    print(f"\nðŸ“Š Target variable: {target_col}")
    print(f"   Valid samples: {df_features[target_col].notna().sum()}")
    print(f"   Mean: Â£{df_features[target_col].mean():.0f}/month")
    
    y = df_features[target_col]
    
    # 5. Prepare features
    X, feature_names = prepare_features(df_features)
    
    # 6. Train model
    model_artifact = train_quantile_model(X, y)
    
    # 7. Evaluate
    metrics = evaluate_model(model_artifact, X, y)
    
    # 8. SHAP explainability
    shap_info = compute_shap_values(model_artifact, X)
    
    # 9. Save model
    save_model(model_artifact, metrics, shap_info)
    
    # 10. Demo prediction
    print("\n" + "=" * 70)
    print("DEMO PREDICTION")
    print("=" * 70)
    
    # Pick a sample row for demo
    sample_idx = df_features[target_col].notna().idxmax()
    sample_features = X.loc[sample_idx].to_dict()
    actual = y.loc[sample_idx]
    district = df_features.loc[sample_idx, "district"]
    
    pred = predict(model_artifact, sample_features)
    
    print(f"\nDistrict: {district}")
    print(f"Actual Rent: Â£{actual:.0f}/month")
    print(f"Predicted:")
    print(f"  P10 (low):    Â£{pred['p10']:.0f}/month")
    print(f"  P50 (median): Â£{pred['p50']:.0f}/month")
    print(f"  P90 (high):   Â£{pred['p90']:.0f}/month")
    
    print("\n" + "=" * 70)
    print("âœ… TRAINING COMPLETE")
    print("=" * 70)
    print(f"""
Model saved to: {MODEL_PATH}

To use in your application:
    
    from Train_Model import load_model, predict
    
    model = load_model()
    result = predict(model, {{
        "total_properties": 25000,
        "growth_latest_avg_price": 500000,
        "crime_total_incidents": 150,
        ...
    }})
    
    print(f"Predicted rent: Â£{{result['p50']:.0f}}/month")
    print(f"Range: Â£{{result['p10']:.0f}} - Â£{{result['p90']:.0f}}")
""")
