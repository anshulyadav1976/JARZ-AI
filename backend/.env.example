# -----------------------------------------------------------------------------
# Backend environment (.env.example)
# -----------------------------------------------------------------------------
# Copy this file to `backend/.env` (DO NOT COMMIT `backend/.env`).
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# OpenRouter (LLM) Configuration (required for chat)
# -----------------------------------------------------------------------------
# Get a key from: https://openrouter.ai/
OPENROUTER_API_KEY=sk-or-v1-REPLACE_ME

# Model (OpenAI-compatible via OpenRouter). Keep small for hackathon demos.
LLM_MODEL=openai/gpt-5-nano
LLM_BASE_URL=https://openrouter.ai/api/v1

# -----------------------------------------------------------------------------
# ScanSan API Configuration (optional)
# -----------------------------------------------------------------------------
# Docs / OpenAPI spec used by this repo:
# - ScanSan docs: https://docs.scansan.com/v1/docs
# - ScanSan OpenAPI spec (in this repo): ./api-1.json
#
# IMPORTANT:
# - Never paste real API keys into `.env.example`.
# - If a real key was ever committed, rotate it immediately.
SCANSAN_API_KEY=REPLACE_ME

# Prefer the base without /v1 (client normalizes paths either way).
SCANSAN_BASE_URL=https://api.scansan.com

# Enable/disable calling ScanSan (recommended default for demo stability).
USE_SCANSAN=false

# -----------------------------------------------------------------------------
# Local chat DB (SQLite) persistence
# -----------------------------------------------------------------------------
# Defaults to `backend/app/chat.db` if unset.
# CHAT_DB_PATH=C:\path\to\chat.db

# -----------------------------------------------------------------------------
# Model Configuration (PLACEHOLDER)
# -----------------------------------------------------------------------------
# Options: stub | local_pickle | http
MODEL_PROVIDER=stub

# For local_pickle provider (PLACEHOLDER)
MODEL_PATH=./models/model.pkl

# For http provider (PLACEHOLDER)
MODEL_HTTP_URL=http://localhost:8001/predict

# -----------------------------------------------------------------------------
# Cache settings (persistent to backend/cache.json)
# -----------------------------------------------------------------------------
ENABLE_CACHE=true
CACHE_TTL_SECONDS=3600
